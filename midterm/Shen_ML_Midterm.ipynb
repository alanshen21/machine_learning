{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning - Mid-term Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Import the spam dataset and print the first six rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "QEdru3RtBmqW",
    "outputId": "eb4d77d9-39ef-4295-9e8b-f0a7ee8cdd85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "5             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "5            1.85             0.00               0.00                 1.85   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "5              0.00             0.00  ...          0.00         0.223   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "5           0.0         0.000         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "5                        3.000                           15   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "5                         54     1  \n",
       "\n",
       "[6 rows x 58 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:/Users/alans/Desktop/QMSS/ML/midterm/spam_dataset.csv\")\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUENsa9wBVLY"
   },
   "source": [
    "### 2.  Read through the documentation of the original dataset here:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
    "\n",
    "The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the variables \"capital_run_length_total\" (total length of uninterrupted sequences of capital letters), \"char_freq_!:\" (percentage of characters in the e-mail that match \"!\"), and \"word_freq_our\" (percentage of words in the e-mail that match the word \"our\") are the most important predictors of spam. Spam emails tend to overuse capital letters, exclamation marks, in order to grab the readers' attention, as well as frequent references to their product or service with the word \"our\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Visualize the univariate distribution of each of the variables in the previous question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23e86d80280>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAFgCAYAAABqo8hyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYNUlEQVR4nO3dfZQldX3n8fcHRh4SEwZ0luCAB1RigvGI7oiIZhfxgZGokF1QPB4dDQZ3Ja5uEl2Je6LBcDY+bDTG9YEoC7KsPGkCsi4cBGSzGkEQQUGREXAZfGAQxGgS4+h3/6hfM5dmerqnp6t/M9Pv1zn3zK9+Vbfqe3/T99PVVbfqpqqQJPWxU+8CJGkpM4QlqSNDWJI6MoQlqSNDWJI6Wta7gDGsXr26Lrnkkt5lSFqasiUL75B7wvfcc0/vEiRpTnbIEJak7YUhLEkdGcKS1JEhLEkdGcKS1JEhLEkdGcKS1JEhLEkdGcKS1JEhLEkdGcKS1JEhLEkdGcKS1JEh3Kzc79EkWZTHyv0e3fvlStpG7JD3E56Pb6+7k5d8+POLsq1zX3PYomxH0rbPPWFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSODGFJ6sgQlqSORg/hJDsnuT7JxW36gCRXJ1mb5Nwku7T+Xdv02jZ//4l1nNz6b0ly5Ng1S9JiWYw94dcDX5uYfgfwnqp6HHAfcELrPwG4r/W/py1HkoOA44EnAKuBDyTZeRHqlqTRjRrCSfYFfgv4SJsOcARwQVvkTOCY1j66TdPmP7stfzRwTlX9pKpuB9YCh4xZtyQtlrH3hN8LvAn4eZt+BPCDqtrQptcBK1t7JXAnQJt/f1v+gf5NPOcBSU5Mcm2Sa9evX7/AL0OSxjFaCCd5AXB3VV031jYmVdVpVbWqqlatWLFiMTYpSVtt2YjrfgbwoiRHAbsBvwz8BbA8ybK2t7svcFdb/i5gP2BdkmXAHsD3J/qnTD5HkrZro+0JV9XJVbVvVe3PcGLtiqp6GXAlcGxbbA1wYWtf1KZp86+oqmr9x7dPTxwAHAhcM1bdkrSYxtwTnsl/As5J8qfA9cBHW/9HgbOSrAXuZQhuquqmJOcBNwMbgJOq6meLX7YkLbxFCeGq+izw2da+jU18uqGq/gk4bobnnwqcOl6FktSHV8xJUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1NFoIJ9ktyTVJbkhyU5I/af0HJLk6ydok5ybZpfXv2qbXtvn7T6zr5NZ/S5Ijx6pZkhbbmHvCPwGOqKonAQcDq5McCrwDeE9VPQ64DzihLX8CcF/rf09bjiQHAccDTwBWAx9IsvOIdUvSohkthGvwozb5sPYo4AjggtZ/JnBMax/dpmnzn50krf+cqvpJVd0OrAUOGatuSVpMox4TTrJzki8DdwOXAd8EflBVG9oi64CVrb0SuBOgzb8feMRk/yaeM7mtE5Ncm+Ta9evXj/BqJGnhjRrCVfWzqjoY2Jdh7/XXRtzWaVW1qqpWrVixYqzNSNKCWpRPR1TVD4ArgacDy5Msa7P2Be5q7buA/QDa/D2A70/2b+I5krRdG/PTESuSLG/t3YHnAl9jCONj22JrgAtb+6I2TZt/RVVV6z++fXriAOBA4Jqx6pakxbRs9kXmbR/gzPZJhp2A86rq4iQ3A+ck+VPgeuCjbfmPAmclWQvcy/CJCKrqpiTnATcDG4CTqupnI9YtSYtmtBCuqhuBJ2+i/zY28emGqvon4LgZ1nUqcOpC1yhJvXnFnCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkeGsCR1ZAhLUkdzCuEkz5hLnyRpy8x1T/gv59gnSdoCyzY3M8nTgcOAFUl+f2LWLwM7j1mYJC0Fmw1hYBfg4W25X5ro/yFw7FhFSdJSsdkQrqqrgKuSnFFV31qkmiRpyZhtT3jKrklOA/affE5VHTFGUZK0VMw1hM8HPgR8BPjZeOVI0tIy1xDeUFUfHLUSSVqC5voRtU8leW2SfZLsNfUYtTJJWgLmuie8pv37xom+Ah6zsOVI0tIypxCuqgPGLkSSlqI5hXCSV2yqv6o+trDlSNLSMtfDEU+daO8GPBv4EmAIS9JWmOvhiNdNTidZDpwzRkGStJTM91aWPwY8TixJW2mux4Q/xfBpCBhu3PPrwHljFSVJS8Vcjwm/e6K9AfhWVa0boR5JWlLmdDii3cjn6wx3UtsT+Ocxi5KkpWKu36zxYuAa4DjgxcDVSbyVpSRtpbkejngL8NSquhsgyQrgM8AFYxUmSUvBXD8dsdNUADff34LnSpJmMNc94UuSXAp8vE2/BPj0OCVJ0tIx23fMPQ7Yu6remOTfAM9ss/4OOHvs4iRpRzfbnvB7gZMBquqTwCcBkjyxzXvhiLVJ0g5vtuO6e1fVV6Z3tr79R6lIkpaQ2UJ4+Wbm7b6AdUjSkjRbCF+b5HendyZ5NXDdOCVJ0tIx2zHhNwB/neRlbAzdVcAuwG+PWJckLQmbDeGq+h5wWJJnAb/Ruv9XVV0xemWStATM9X7CVwJXjlyLJC05XvUmSR0ZwpLU0WghnGS/JFcmuTnJTUle3/r3SnJZklvbv3u2/iR5X5K1SW5M8pSJda1py9+aZM1YNUvSYhtzT3gD8AdVdRBwKHBSkoOANwOXV9WBwOVtGuD5wIHtcSLwQRhCG3gr8DTgEOCtU8EtSdu70UK4qr5TVV9q7b8HvgasBI4GzmyLnQkc09pHAx+rwReA5Un2AY4ELquqe6vqPuAyYPVYdUvSYlqUY8JJ9geeDFzNcCn0d9qs7wJ7t/ZK4M6Jp61rfTP1T9/GiUmuTXLt+vXrF/YFSNJIRg/hJA8HPgG8oap+ODmvqoqNXyC6VarqtKpaVVWrVqxYsRCrlKTRjRrCSR7GEMBnt7uwAXyvHWag/Tt1s/i7gP0mnr5v65upX5K2e2N+OiLAR4GvVdWfT8y6CJj6hMMa4MKJ/le0T0kcCtzfDltcCjwvyZ7thNzzWp8kbffm+s0a8/EM4OXAV5J8ufX9EfBnwHlJTgC+xfDFoTB8U8dRwFrgH4BXAVTVvUneDnyxLXdKVd07Yt2StGhGC+Gq+r9AZpj97E0sX8BJM6zrdOD0hatOkrYNXjEnSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLUkSEsSR0ZwpLU0WghnOT0JHcn+epE315JLktya/t3z9afJO9LsjbJjUmeMvGcNW35W5OsGateSephzD3hM4DV0/reDFxeVQcCl7dpgOcDB7bHicAHYQht4K3A04BDgLdOBbck7QhGC+Gq+j/AvdO6jwbObO0zgWMm+j9Wgy8Ay5PsAxwJXFZV91bVfcBlPDTYJWm7tdjHhPeuqu+09neBvVt7JXDnxHLrWt9M/ZK0Q+h2Yq6qCqiFWl+SE5Ncm+Ta9evXL9RqJWlUix3C32uHGWj/3t367wL2m1hu39Y3U/9DVNVpVbWqqlatWLFiwQuXpDEsdghfBEx9wmENcOFE/yvapyQOBe5vhy0uBZ6XZM92Qu55rU+SdgjLxlpxko8DhwOPTLKO4VMOfwacl+QE4FvAi9vinwaOAtYC/wC8CqCq7k3yduCLbblTqmr6yT5J2m6NFsJV9dIZZj17E8sWcNIM6zkdOH0BS5OkbYZXzElSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCPew0zKSjP5Yud+je79SSbNY1ruAJennG3jJhz8/+mbOfc1ho29D0tZxT1iSOtpuQjjJ6iS3JFmb5M2965GkhbBdhHCSnYH/BjwfOAh4aZKD+la1HfDYs7TN216OCR8CrK2q2wCSnAMcDdzctaptnceepW1eqqp3DbNKciywuqpe3aZfDjytqn5vYpkTgRPb5OOBW7ZwM48E7lmAcreGNVjDdNtCHdawZTXcU1Wr57rS7WVPeFZVdRpw2nyfn+Taqlq1gCVZgzXsEHVYw7g1bBfHhIG7gP0mpvdtfZK0XdteQviLwIFJDkiyC3A8cFHnmiRpq20XhyOqakOS3wMuBXYGTq+qmxZ4M/M+lLGArGFgDRttC3VYw2CUGraLE3OStKPaXg5HSNIOyRCWpI4MYca9JDrJfkmuTHJzkpuSvL7175XksiS3tn/3bP1J8r5Wy41JnjKxrjVt+VuTrNnCOnZOcn2Si9v0AUmubts5t53wJMmubXptm7//xDpObv23JDlyHmOxPMkFSb6e5GtJnt5hHP5j+3/4apKPJ9lt7LFIcnqSu5N8daJvwV53kn+Z5CvtOe9LkjnW8K72f3Fjkr9Osny21zfTe2WmMZythol5f5Ckkjxysceh9b+ujcVNSd455jg8RFUt6QfDib5vAo8BdgFuAA5awPXvAzyltX8J+AbDpdfvBN7c+t8MvKO1jwL+NxDgUODq1r8XcFv7d8/W3nML6vh94H8CF7fp84DjW/tDwL9v7dcCH2rt44FzW/ugNja7Age0Mdt5C8fiTODVrb0LsHwxxwFYCdwO7D4xBq8ceyyAfwU8BfjqRN+CvW7gmrZs2nOfP8cangcsa+13TNSwydfHZt4rM43hbDW0/v0YTrp/C3hkh3F4FvAZYNc2/S/GHIeH1LRQYbO9PoCnA5dOTJ8MnDzi9i4EnstwRd8+rW8f4JbW/jDw0onlb2nzXwp8eKL/QcvNss19gcuBI4CL2w/pPRNvwAfGoL0Znt7ay9pymT4uk8vNsYY9GAIw0/oXcxxWAne2N/CyNhZHLsZYAPtPe+MvyOtu874+0f+g5TZXw7R5vw2cvan3wNTrY4b3yuZ+nuZSA3AB8CTgDjaG8KKNA0NwPmcTy402DpMPD0dsfGNOWdf6Flz7c/bJwNXA3lX1nTbru8Des9SzNXW+F3gT8PM2/QjgB1W1YRPremA7bf79bfmtHacDgPXAf89wWOQjSX6RRRyHqroLeDfw/4DvMLy261j8sYCFe90rW3tragH4HYa9x/nUsLmfp81KcjRwV1XdMG3WYo7DrwK/2Q4jXJXkqfOsYV7jYAgvkiQPBz4BvKGqfjg5r4Zfm6N8VjDJC4C7q+q6Mda/BZYx/Bn4wap6MvBjhj/DHzDmOAC0465HM/xCeBTwi8Ccr/Efy9ivezZJ3gJsAM5e5O3+AvBHwB8v5nY3YRnDX0eHAm8EztvU8eSxGMKLcEl0kocxBPDZVfXJ1v29JPu0+fsAd89Sz3zrfAbwoiR3AOcwHJL4C2B5kqmLdSbX9cB22vw9gO9vxfanrAPWVdXVbfoChlBerHEAeA5we1Wtr6qfAp9kGJ/FHgtYuNd9V2vPq5YkrwReALys/TKYTw3fZ+Yx3JzHMvxCvKH9fO4LfCnJr8yjhq0Zh3XAJ2twDcNfjI+cRw3zG4e5HEvbkR8MvwVvY/hhmDrI/oQFXH+AjwHvndb/Lh58Yuadrf1bPPiExDWtfy+GY6p7tsftwF5bWMvhbDwxdz4PPoHw2tY+iQefjDqvtZ/Ag09S3MaWn5j7W+Dxrf22NgaLNg7A04CbgF9o6z0TeN1ijAUPPQ65YK+bh56QOmqONaxmuB3simnLbfL1sZn3ykxjOFsN0+bdwcZjwos5Dv8OOKW1f5XhUEPGHIcH1bNQYbM9PxjOxH6D4YznWxZ43c9k+FPzRuDL7XEUw/Gjy4FbGc7MTv0gheEG9t8EvgKsmljX7wBr2+NV86jlcDaG8GPaD+3a9oMzdWZ4tza9ts1/zMTz39LquoVNnHmew/YPBq5tY/E37U20qOMA/AnwdeCrwFntDTbqWAAfZzgG/VOGva4TFvJ1A6va6/km8H6mnfzcTA1rGQJn6ufyQ7O9PmZ4r8w0hrPVMG3+HWwM4cUch12A/9Ge+yXgiDHHYfrDy5YlqSOPCUtSR4awJHVkCEtSR4awJHVkCEtSR4awJHVkCGt0SR6V5ILWPjjJUXN4zuFpt91cTEneluQPR1jvG9plulPTP9qC5x6T5KA5LPfKJI+aw3JnJDl2rtvXuAxhja6qvl1VU2/6gxk+6L5gJi4T3Za9geFKvfk4huG2irN5JcM9MbQdMYQ1qySvaDfWviHJWUle2O44dX2SzyTZuy33tjb/79oNt3+39e+f4SbquwCnAC9J8uUkL0lySFv++iSfT/L4OdY0ta3PAWe1vcD3T8y/OMnhrf2jJKe2+r8wVe8ctvHYJJckuS7J3yb5tdZ/Rrtp+OeT3Da1V5lkpyQfaDcHvyzJp5Mcm+Q/MITjlUmunFj/rDUlOQx4EfCuNmaPbX9NfCEbb8a+Z6thFXB2W273JH+c5Itt7E9bzJvSaAss5CW6Pna8B8P1899g4+WkUzfTnrra8tXAf23ttzFcR787ww1Q7mQIn/1p1+oz7K29f2L9v8zG+68+B/hEax9Ou8R6hrrexnAbyt1nWO/FwOGtXcALW/udwH+eZb1/2NqXAwe29tOAK1r7DIZLUndi2ENd2/qPBT7d+n8FuA84ts27Y2oM51HTGVPradM3Av+6tU+h3ZcE+CwPvrx3r4n2WRPbe2B97fkv6v1ztpQf28OfcerrCOD8qroHoKruTfJE4Nx2969dGG6iMuXCqvpH4B/bXt8hDPclmMkewJlJDmQIpodtQW0XtW3N5p8ZQhmG4H7ubE9otx49DDh/Ygdy14lF/qaqfg7cPLEX+0yGsfo58N3Jvd6FqKnVtQewvKqual1nMvxC2JRnJXkTw2GQvRhuXvSpyQWqqvdtJJc8D0doPv6SYa/zicBrGG50M2X6zUhmuznJ24Erq+o3gBdOW9dsfjzR3sCDf54n1/PTart9wM9gTjsfOzHcoPvgicevT8z/yUR7Pn/mz6emOUuyG/ABhj3eJwJ/xZaNrRaJIazZXAEcl+QRMHxBJcPe69R9UtdMW/7oDF+e+QiGQwpfnDb/7xm+a2/K5LpeuRV13gEc3I7L7sewBz5vNdx4//Ykx8EDXzz5pFme9jng37Ya9mZ4/VOmv+4t8cBzq+p+4L4kv9nmvRy4avpybAzce9pevZ+G2EYZwtqsqroJOBW4KskNwJ8zHDc9P8l1DN+pNelG4ErgC8Dbq+rb0+ZfCRw0dWKO4Xjof0lyPVu3N/g5hsMiNwPvY7gl4dZ6GXBCe903MXwrx+Z8guH2iDcz3BrxSwxfiQRwGnDJLIcoZnIO8MZ28vKxDL/43pXkRoZPm5zSljsD+FCSLzPsqf8Vw+0ZL+WhvwwBSHJKkhfNoyYtEG9lqQWT5G3Aj6rq3b1r6SXJw6vqR+0vgWuAZ1TVd3vXpW2XJ+akhXVxkuUMJyzfbgBrNu4Ja5uW5FXA66d1f66qTtrK9b4FOG5a9/lVderWrHdrbIs1aXyGsCR15Ik5SerIEJakjgxhSerIEJakjv4/3dL/K2DgBdwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting\n",
    "import seaborn as sns\n",
    "\n",
    "sns.displot(data, x='capital_run_length_total:', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23e86d148b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAT10lEQVR4nO3df7BndV3H8efLXRFHygW9wzDLGpRMhmZqGyo6jsKEq5WLBYiZrIXiTNjo9MsfNVEajf1Ssx8YCcPimAuhBpUjbYBaY4IrP4WV2EyG3ZBdXNDI0Vp498f3c+vbunf3snvP/Xy/e5+PmTv3nPc533Pe98y9rz37+Z5zvqkqJEmL7zG9G5CkpcoAlqRODGBJ6sQAlqRODGBJ6mR57waGsGbNmvrkJz/Zuw1JmpU9FQ/KM+D777+/dwuStE8HZQBL0jQwgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgMesXPUUkgz+tXLVU3r/qJImwEH5POD99e9b7+FVf/7Zwfdz2RtPHHwfkiafZ8CS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1MngAZxkWZKbkvxtmz82yfVJtiS5LMkhrf64Nr+lLT9mbBtvb/U7k7x06J4laTEsxhnwm4HNY/O/C7y3qp4KPACc3epnAw+0+nvbeiQ5HjgTeDqwBvizJMsWoW9JGtSgAZzkaODHgA+2+QAnAVe0VdYDp7bptW2etvzktv5aYENVfbuq/g3YApwwZN+StBiGPgN+H/CrwCNt/knAg1W1q81vBVa26ZXAPQBt+dfb+v9b38Nr/leSc5JsSrJpx44dC/xjSNLCGyyAk/w4sL2qvjDUPsZV1YVVtbqqVs/MzCzGLiXpgCwfcNsvAF6R5OXAocB3A38ErEiyvJ3lHg1sa+tvA1YBW5MsB54IfG2sPmv8NZI0tQY7A66qt1fV0VV1DKM30a6tqtcA1wGntdXWAVe26avaPG35tVVVrX5mu0riWOA44Iah+pakxTLkGfBc3gpsSPLbwE3ARa1+EfChJFuAnYxCm6q6PcnlwB3ALuDcqnp48duWpIW1KAFcVZ8CPtWmv8wermKoqm8Bp8/x+vOB84frUJIWn3fCSVInBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1IngwVwkkOT3JDkliS3J/mtVj82yfVJtiS5LMkhrf64Nr+lLT9mbFtvb/U7k7x0qJ4laTENeQb8beCkqvoh4FnAmiTPA34XeG9VPRV4ADi7rX828ECrv7etR5LjgTOBpwNrgD9LsmzAviVpUQwWwDXyUJt9bPsq4CTgilZfD5zapte2edryk5Ok1TdU1ber6t+ALcAJQ/UtSYtl0DHgJMuS3AxsBzYC/wo8WFW72ipbgZVteiVwD0Bb/nXgSeP1PbxmfF/nJNmUZNOOHTsG+GkkaWENGsBV9XBVPQs4mtFZ69MG3NeFVbW6qlbPzMwMtRtJWjCLchVEVT0IXAc8H1iRZHlbdDSwrU1vA1YBtOVPBL42Xt/DayRpag15FcRMkhVt+vHAjwKbGQXxaW21dcCVbfqqNk9bfm1VVauf2a6SOBY4DrhhqL4labEs3/cq++0oYH27YuExwOVV9bdJ7gA2JPlt4Cbgorb+RcCHkmwBdjK68oGquj3J5cAdwC7g3Kp6eMC+JWlRDBbAVXUr8Ow91L/MHq5iqKpvAafPsa3zgfMXukdJ6sk74SSpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjqZVwAnecF8apKk+ZvvGfAfz7MmSZqnvX4sfZLnAycCM0l+cWzRdwPLhmxMkg52ew1g4BDgsLbed43VvwGcNlRTkrQU7DWAq+rTwKeTXFJVdy9ST5K0JOzrDHjW45JcCBwz/pqqOmmIpiRpKZhvAP8V8AHgg8DDw7UjSUvHfAN4V1VdMGgnkrTEzPcytL9J8vNJjkpyxOzXoJ1J0kFuvmfA69r3XxmrFfC9C9uOJC0d8wrgqjp26EYkaamZVwAnOWtP9aq6dGHbkaSlY75DED8yNn0ocDJwI2AAS9J+mu8QxC+MzydZAWwYoiFJWir293GU/wk4LixJB2C+Y8B/w+iqBxg9hOcHgMuHakqSloL5jgH/wdj0LuDuqto6QD+StGTMawiiPZTnS4yeiHY48F9DNiVJS8F8PxHjDOAG4HTgDOD6JD6OUpIOwHyHIH4N+JGq2g6QZAb4B+CKoRqTpIPdfK+CeMxs+DZfexSvlSTtwXzPgD+Z5GrgI23+VcAnhmlJkpaGfX0m3FOBI6vqV5L8JPDCtuifgQ8P3ZwkHcz2dQb8PuDtAFX1MeBjAEl+sC37iQF7k6SD2r7GcY+sqtt2L7baMYN0JElLxL4CeMVelj1+AfuQpCVnXwG8Kckbdi8meT3whWFakqSlYV9jwG8BPp7kNfxf4K4GDgFeOWBfknTQ22sAV9V9wIlJXgI8o5X/rqquHbwzSTrIzfd5wNcB1w3ciyQtKd7NJkmdGMCS1IkBLEmdGMCS1IkBLEmdDBbASVYluS7JHUluT/LmVj8iycYkd7Xvh7d6krw/yZYktyZ5zti21rX170qybqieJWkxDXkGvAv4pao6HngecG6S44G3AddU1XHANW0e4GXAce3rHOACGAU2cB7wXOAE4LzZ0JakaTZYAFfVvVV1Y5v+D2AzsBJYC6xvq60HTm3Ta4FLa+RzwIokRwEvBTZW1c6qegDYCKwZqm9JWiyLMgac5Bjg2cD1jJ6wdm9b9FXgyDa9Erhn7GVbW22u+u77OCfJpiSbduzYsbA/gCQNYPAATnIY8FHgLVX1jfFlVVVALcR+qurCqlpdVatnZmYWYpOSNKhBAzjJYxmF74fbA90B7mtDC7Tvs581tw1YNfbyo1ttrrokTbUhr4IIcBGwuareM7boKmD2SoZ1wJVj9bPa1RDPA77ehiquBk5Jcnh78+2UVpOkqTbfD+XcHy8AXgvcluTmVnsH8G7g8iRnA3cDZ7RlnwBeDmwBvgn8LEBV7UzyLuDzbb13VtXOAfuWpEUxWABX1T8BmWPxyXtYv4Bz59jWxcDFC9edJPXnnXCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1IkBLEmdGMCS1MlgAZzk4iTbk3xxrHZEko1J7mrfD2/1JHl/ki1Jbk3ynLHXrGvr35Vk3VD9StJiG/IM+BJgzW61twHXVNVxwDVtHuBlwHHt6xzgAhgFNnAe8FzgBOC82dCWpGk3WABX1WeAnbuV1wLr2/R64NSx+qU18jlgRZKjgJcCG6tqZ1U9AGzkO0NdkqbSYo8BH1lV97bprwJHtumVwD1j621ttbnq3yHJOUk2Jdm0Y8eOhe1akgbQ7U24qiqgFnB7F1bV6qpaPTMzs1CblaTBLHYA39eGFmjft7f6NmDV2HpHt9pcdUmaeosdwFcBs1cyrAOuHKuf1a6GeB7w9TZUcTVwSpLD25tvp7SaJE295UNtOMlHgBcDT06yldHVDO8GLk9yNnA3cEZb/RPAy4EtwDeBnwWoqp1J3gV8vq33zqra/Y09SZpKgwVwVb16jkUn72HdAs6dYzsXAxcvYGuSNBG8E06SOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOjGAJakTA1iSOpmaAE6yJsmdSbYkeVvvfiTpQE1FACdZBvwp8DLgeODVSY7v29UBeMxykizK18pVT+n900qaw/LeDczTCcCWqvoyQJINwFrgjq5d7a9HdvGqP//souzqsjeeuCj7kfTopap697BPSU4D1lTV69v8a4HnVtWbxtY5BzinzX4/cOd+7OrJwP0H2O7QJr3HSe8P7HGh2OP83V9Va3YvTssZ8D5V1YXAhQeyjSSbqmr1ArU0iEnvcdL7A3tcKPZ44KZiDBjYBqwamz+61SRpak1LAH8eOC7JsUkOAc4ErurckyQdkKkYgqiqXUneBFwNLAMurqrbB9jVAQ1hLJJJ73HS+wN7XCj2eICm4k04SToYTcsQhCQddAxgSerEAGY6bnNO8pUktyW5Ocmm3v0AJLk4yfYkXxyrHZFkY5K72vfDJ7DH30yyrR3Lm5O8vHOPq5Jcl+SOJLcneXOrT8yx3EuPE3Mskxya5IYkt7Qef6vVj01yffv7vqy9kT8RlvwYcLvN+V+AHwW2Mrri4tVVNVF32SX5CrC6qibhonIAkrwIeAi4tKqe0Wq/B+ysqne3f8wOr6q3TliPvwk8VFV/0KuvcUmOAo6qqhuTfBfwBeBU4HVMyLHcS49nMCHHMkmAJ1TVQ0keC/wT8GbgF4GPVdWGJB8AbqmqC3r2Ossz4LHbnKvqv4DZ25y1D1X1GWDnbuW1wPo2vZ7RH2k3c/Q4Uarq3qq6sU3/B7AZWMkEHcu99DgxauShNvvY9lXAScAVrd79d3KcATz6JbpnbH4rE/aL1RTw90m+0G67nlRHVtW9bfqrwJE9m9mLNyW5tQ1RdB0mGZfkGODZwPVM6LHcrUeYoGOZZFmSm4HtwEbgX4EHq2pXW2Wi/r4N4Onxwqp6DqMnwp3b/ms90Wo0vjWJY1wXAN8HPAu4F/jDrt00SQ4DPgq8paq+Mb5sUo7lHnqcqGNZVQ9X1bMY3S17AvC0nv3siwE8Jbc5V9W29n078HFGv1yT6L42Xjg7bri9cz/foarua3+ojwB/wQQcyzZm+VHgw1X1sVaeqGO5px4n8VgCVNWDwHXA84EVSWZvOpuov28DeApuc07yhPbGB0meAJwCfHHvr+rmKmBdm14HXNmxlz2aDbXmlXQ+lu3No4uAzVX1nrFFE3Ms5+pxko5lkpkkK9r04xm9sb6ZURCf1labqN/JJX8VBEC7dOZ9/N9tzuf37ej/S/K9jM56YXT7+F9OQo9JPgK8mNEj/+4DzgP+GrgceApwN3BGVXV7E2yOHl/M6L/MBXwFeOPYWOuiS/JC4B+B24BHWvkdjMZYJ+JY7qXHVzMhxzLJMxm9ybaM0cnl5VX1zvb3swE4ArgJ+Jmq+naPHndnAEtSJw5BSFInBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAa2oluSTJafte81Fv9/Qkm5Nct9DbHtvH65L8yVDb13QwgLUkZWSu3/+zgTdU1Ut2e81UfIaipocBrKmR5Kz21K1bknyolV+U5LNJvjx7NpzksCTXJLkxo4fYr231YzJ68P6ljG6ZXbWHffwG8ELgoiS/385Ur0pyLXBNuy384vbg75vGtv34JBvamfPH2wPAVy/GcdH08l90TYUkTwd+HTixqu5PcgTwHuAoRoH5NEbPTrgC+Bbwyqr6RpInA59LMvt8j+OAdVX1uT3tp926ehLwy1W1KcnrgOcAz6yqnUl+B7i2qn6uPXfghiT/ALwR+GZV/UC7JfbGR/nzfRD4QFVNxKedaHEYwJoWJwF/NfuJIC0MAf66PYnrjiSzz8sN8DvtkZ2PMHr+6+yyu+cK373YOPYMhlOAVyT55TZ/KKNnNbwIeH/r7dYktz6aHVTV6x9lTzoIGMCaduMPVUn7/hpgBvjhqvrv9nFOh7Zl/7kf+xh/TYCfqqo7x1do/xjMW1VdAlyyH73oIOIYsKbFtcDpSZ4Eow+s3Mu6TwS2t/B9CfA9C9jH1cAvtMczkuTZrf4Z4Kdb7RnAMxdwnzpIGcCaClV1O3A+8OkktzAa/53Lh4HVSW4DzgK+tICtvIvRZ43dmuT2Ng+jT4Y4LMlm4J2MPrRyTklekbFP4E7yQd+0W3p8HKU0gCSfor2R17sXTS7PgCWpE8+AtWQluR543G7l11bVbdO0D00vA1iSOnEIQpI6MYAlqRMDWJI6MYAlqZP/AW3DSedICAbTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data,x='char_freq_!:', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x23e99ce2af0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data, x='word_freq_our:', bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning models that are used to predict classification variables like spam include: k-nearest neighbors, logistic regression, penalized logistic regression, support-vector machines, decision trees, bagged trees, and random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train-test split is used to evaluate the performance of machine learning algorithms. When a data set is separated into a training set and testing set, most of the data is used for training, and a smaller portion of the data is used for testing. The main objective is to generate a training set that is aptly fitted and can generalize well to new data. After a model has been processed with the training set, the model is tested by making predictions against the test set. The accuracy of the model's predictions are then easily determined since the testing set already consists of known values from the training set. Training and testing with similar data prevents overfitting and underfitting, and helps us achieve a better understanding of the characteristics of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is k-fold cross validation and what do we use it for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A k-fold cross validation produces multiple('k' number of) train-test sets instead of 1. Essentially, a k-fold CV trains a model k-times and also tests it k-times. It attempts to maximize the use of available data for training and testing a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How is k-fold cross validation different from stratified k-fold cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold CV divides the dataset into k folds. Stratified k-fold returns stratified folds made by preserving the percentage of samples for each class to ensure that each fold of dataset has the same proportion of observations. Stratified k-fold cross validation is typically more useful when the size of the data is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2259</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>191</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   capital_run_length_total:  char_freq_!:  word_freq_our:\n",
       "0                        278         0.778            0.32\n",
       "1                       1028         0.372            0.14\n",
       "2                       2259         0.276            1.23\n",
       "3                        191         0.137            0.63\n",
       "4                        191         0.135            0.63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['spam']\n",
    "cols = data.loc[:, ['capital_run_length_total:', 'char_freq_!:', 'word_freq_our:']] \n",
    "X = cols\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "X.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Training set score: 0.768\n",
      "Test set score: 0.774\n",
      "Mean Cross Validation, KFold: 0.768\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression(penalty = 'none').fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "# Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train, y_train))))\n",
    "\n",
    "# prediction\n",
    "logreg_predicted_vals = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal logistic regression with penalty as \"none\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (UNSCALED)\n",
      "Training set score: 0.768\n",
      "Test set score: 0.774\n",
      "Mean Cross Validation, KFold: 0.768\n"
     ]
    }
   ],
   "source": [
    "#Penalized Logistic Regression with L1\n",
    "\n",
    "logreg_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train, y_train) \n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (UNSCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg_l1.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_l1.score(X_test, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (SCALED)\n",
      "Training set score: 0.768\n",
      "Test set score: 0.774\n",
      "Mean Cross Validation, KFold: 0.768\n"
     ]
    }
   ],
   "source": [
    "#Scaling with StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Scaled Penalized Logistic Regression with L1\n",
    "logreg_scaled_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (SCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg_scaled_l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_scaled_l1.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Cross Validation \n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg_scaled_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7681159420289856\n",
      "{'C': 10}\n",
      "0.7741094700260643\n"
     ]
    }
   ],
   "source": [
    "#Tuning with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid = GridSearchCV(LogisticRegression(penalty = 'l1', solver = 'liblinear'), param_grid)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (SCALED)\n",
      "Training set score: 0.768\n",
      "Test set score: 0.774\n",
      "Mean Cross Validation, KFold: 0.768\n"
     ]
    }
   ],
   "source": [
    "#Running L1 Logreg with suggested parameter\n",
    "logreg_scaled_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=10).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (SCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg_scaled_l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_scaled_l1.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Cross Validation \n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg_scaled_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling with StandardScaler and tuning paramteres with GridSearchCV did not improve the scores for the model. The penalized logistic regression with L1 model returned the exact same results as the regular logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CLASSIFER (UNSCALED)\n",
      "Training set score: 0.828\n",
      "Test set score: 0.738\n",
      "Mean Cross Validation, KFold: 0.742\n"
     ]
    }
   ],
   "source": [
    "#KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFER (UNSCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train))))\n",
    "\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CLASSIFIER (SCALED)\n",
      "Training set score: 0.876\n",
      "Test set score: 0.843\n",
      "Mean Cross Validation, KFold: 0.834\n"
     ]
    }
   ],
   "source": [
    "#Scaled KNN\n",
    "knn_scaled = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFIER (SCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8353623188405799\n",
      "{'n_neighbors': 7}\n",
      "0.8410078192875761\n"
     ]
    }
   ],
   "source": [
    "#Tuning with GridSearchCV\n",
    "param_grid = {'n_neighbors':np.arange(1, 15, 2)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CLASSIFIER (SCALED)\n",
      "Training set score: 0.868\n",
      "Test set score: 0.841\n",
      "Mean Cross Validation, KFold: 0.835\n"
     ]
    }
   ],
   "source": [
    "#Running model with suggested parameter\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=7).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFIER (SCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-value of 7 was determined through GridSearchCV tuning as the best parameter. The K-Neighbors Classifier model improved significantly after scaling and tuning. It performed better than the previous two logistic regression models in terms of test and cross validation scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  Did this model predict test data better than your previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE CLASSIFIER\n",
      "Training set score: 0.968\n",
      "Test set score: 0.829\n",
      "Mean Cross Validation, KFold: 0.822\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "\n",
    "print(\"DECISION TREE CLASSIFIER\")\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(tree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8533333333333333\n",
      "{'criterion': 'entropy', 'max_depth': 7, 'min_samples_leaf': 20}\n",
      "0.8505647263249348\n"
     ]
    }
   ],
   "source": [
    "#Tuning\n",
    "\n",
    "param_grid = {'max_depth':np.arange(1, 32), 'min_samples_leaf':[5, 10, 20, 50, 100], 'criterion':['gini', 'entropy']}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE CLASSIFIER\n",
      "Training set score: 0.869\n",
      "Test set score: 0.851\n",
      "Mean Cross Validation, KFold: 0.854\n"
     ]
    }
   ],
   "source": [
    "#Running model with suggested parameters\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth= 7, min_samples_leaf=20).fit(X_train,y_train)\n",
    "\n",
    "print(\"DECISION TREE CLASSIFIER\")\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "# Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(tree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning with GridSearchCV and determining the best parameters, the Decision Tree Classifier model with tuned hyperparameters returned better test and KFold CV scores than the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61</td>\n",
       "      <td>3.756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1028</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.94</td>\n",
       "      <td>101</td>\n",
       "      <td>5.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2259</td>\n",
       "      <td>0.276</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>485</td>\n",
       "      <td>9.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>191</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>40</td>\n",
       "      <td>3.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>40</td>\n",
       "      <td>3.537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   capital_run_length_total:  char_freq_!:  word_freq_our:  word_freq_mail:  \\\n",
       "0                        278         0.778            0.32             0.00   \n",
       "1                       1028         0.372            0.14             0.94   \n",
       "2                       2259         0.276            1.23             0.25   \n",
       "3                        191         0.137            0.63             0.63   \n",
       "4                        191         0.135            0.63             0.63   \n",
       "\n",
       "   capital_run_length_longest:  capital_run_length_average:  \n",
       "0                           61                        3.756  \n",
       "1                          101                        5.114  \n",
       "2                          485                        9.821  \n",
       "3                           40                        3.537  \n",
       "4                           40                        3.537  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add three new variables\n",
    "\n",
    "y = data['spam']\n",
    "cols = data.loc[:, ['capital_run_length_total:', 'char_freq_!:', 'word_freq_our:', \n",
    "                    'word_freq_mail:', 'capital_run_length_longest:', 'capital_run_length_average:']] \n",
    "X_new = cols\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=42)\n",
    "\n",
    "X_new.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE CLASSIFIER (NEW)\n",
      "Training set score: 0.995\n",
      "Test set score: 0.843\n",
      "Mean Cross Validation, KFold: 0.834\n"
     ]
    }
   ],
   "source": [
    "#I picked the Decision Tree model based on it having the best scores\n",
    "tree = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "\n",
    "print(\"DECISION TREE CLASSIFIER (NEW)\")\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(tree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855072463768116\n",
      "{'criterion': 'entropy', 'max_depth': 7, 'min_samples_leaf': 5}\n",
      "0.8479582971329279\n"
     ]
    }
   ],
   "source": [
    "#Tuning\n",
    "\n",
    "param_grid = {'max_depth':np.arange(1, 32), 'min_samples_leaf':[5, 10, 20, 50, 100], 'criterion':['gini', 'entropy']}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE CLASSIFIER\n",
      "Training set score: 0.894\n",
      "Test set score: 0.851\n",
      "Mean Cross Validation, KFold: 0.856\n"
     ]
    }
   ],
   "source": [
    "#Running model with suggested parameters\n",
    "tree = DecisionTreeClassifier(criterion='entropy', max_depth= 7, min_samples_leaf=5).fit(X_train,y_train)\n",
    "\n",
    "print(\"DECISION TREE CLASSIFIER\")\n",
    "print(\"Training set score: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "\n",
    "# Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(tree, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the new Decision Tree model with added variables produced a slightly better CV score compared to the old model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Rerun all your other models with this final set of six variables, evaluate prediction error, and choose a final model.  Why did you select this model among all of the models that you ran?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION (NEW)\n",
      "Training set score: 0.783\n",
      "Test set score: 0.789\n",
      "Mean Cross Validation, KFold: 0.783\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "logreg = LogisticRegression(penalty = 'none', max_iter=10000).fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (NEW)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
    "\n",
    "# Kfold CV\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train, y_train))))\n",
    "\n",
    "# prediction from test set\n",
    "logreg_predicted_vals = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (NEW SCALED)\n",
      "Training set score: 0.783\n",
      "Test set score: 0.789\n",
      "Mean Cross Validation, KFold: 0.783\n"
     ]
    }
   ],
   "source": [
    "#Scaled Penalized Logistic Regression with L1\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "logreg_scaled_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (NEW SCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg_scaled_l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_scaled_l1.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation \n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg_scaled_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7834782608695653\n",
      "{'C': 1}\n",
      "0.788879235447437\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV\n",
    "param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid = GridSearchCV(LogisticRegression(penalty = 'l1', solver = 'liblinear'), param_grid)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENALIZED LOGISTIC REGRESSION - L1 (NEW SCALED)\n",
      "Training set score: 0.783\n",
      "Test set score: 0.789\n",
      "Mean Cross Validation, KFold: 0.783\n"
     ]
    }
   ],
   "source": [
    "#Running model with suggested C parameter\n",
    "logreg_scaled_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear', C=1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION - L1 (NEW SCALED)\")\n",
    "print(\"Training set score: {:.3f}\".format(logreg_scaled_l1.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(logreg_scaled_l1.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation \n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(logreg_scaled_l1, X_train, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CLASSIFER (SCALED NEW)\n",
      "Training set score: 0.887\n",
      "Test set score: 0.848\n",
      "Mean Cross Validation, KFold: 0.840\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn_scaled = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFER (SCALED NEW)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8420289855072463\n",
      "{'n_neighbors': 9}\n",
      "0.843614248479583\n"
     ]
    }
   ],
   "source": [
    "#Tuning with GridSearchCV\n",
    "param_grid = {'n_neighbors':np.arange(1, 15, 2)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CLASSIFER (SCALED NEW)\n",
      "Training set score: 0.869\n",
      "Test set score: 0.844\n",
      "Mean Cross Validation, KFold: 0.842\n"
     ]
    }
   ],
   "source": [
    "#Running model with suggested k-value\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=9).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFER (SCALED NEW)\")\n",
    "print(\"Training set score: {:.3f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.3f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing all the models I would choose the Decision Tree Classifier model considering that it returned a test score of 0.851 and CV score of 0.856, which are the best scores out of the models that were tested. Also, it is worth noting that the training and test set scores were similar, which is generally a good indication that there is little possibility of overfitting or underfitting. Therefore I think the Decision Tree model represents the best fit in this case and has the ability to return the most accurate predictors of spam email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable that is missing from the data that would improve the model would be the amount or prevalence of links/urls within the email, since a lot of spam emails try to get the reader to click on links that would redirect them to another page. The inclusion of a variable that reflects the number of links within an email would most definitely improve the predictive power of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning models that are used to evaluate continuous variables include: k-nearest neighbors regressor, linear regression, lasso regression, ridge regression, support-vector machines, decision trees, bagged trees, and random forest."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML Midterm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
